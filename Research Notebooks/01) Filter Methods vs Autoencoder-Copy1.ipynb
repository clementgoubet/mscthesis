{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods vs Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of performance of Logistic Regression with or without PCA/SVD for different sets of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import methodsMLinterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = ['DNB', 'NRG', 'CL', 'ANTM', 'NEE', 'PAYX', 'VAR', 'NI', 'MNST', 'JNJ', 'TGNA', 'NOV', 'FIS', 'BLK', 'HBI', 'NVDA', 'DLTR', 'MRO', 'EMN', 'AMT', 'FLR', 'IBM', 'BK', 'NFX', 'AGN', 'LRCX', 'DIS', 'LH', 'C', 'MNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_1p2_extra = ['acr', 'aeo', 'adl', 'aep', 'acy', 'aez', 'afa', 'aab', 'zkg', 'zmd', 'zla', 'zme', 'zkn', 'zmo', 'zmp', 'zhq', 'zpe']\n",
    "features_1p4 = ['aab', 'aac', 'aad', 'aae', 'aaf', 'aag', 'aah', 'abj', 'abm', 'abn', 'abo', 'abp', 'abq', 'abr', 'abs', 'abt', 'abu', 'abv', 'abw', 'abx', 'aby', 'abz', 'aca', 'acb', 'acc', 'acd', 'ace', 'acf', 'acr', 'acw', 'acx', 'acy', 'adi', 'adj', 'adl', 'ado', 'adp', 'adq', 'adr', 'ads', 'adt', 'adu', 'adv', 'adw', 'adx', 'ady', 'adz', 'aea', 'aeb', 'aec', 'aed', 'aee', 'aef', 'aeg', 'aeh', 'aei', 'aej', 'aek', 'ael', 'aem', 'aen', 'aeo', 'aep', 'aeq', 'aer', 'aes', 'aex', 'aey', 'aez', 'afa', 'afj', 'afl', 'afo', 'afp', 'afq', 'afr', 'afs', 'aft', 'afu', 'afv', 'afw', 'afx', 'afy', 'afz', 'aga', 'agb', 'agc', 'agd', 'age', 'agf', 'agg', 'agh', 'agi', 'agj', 'agk', 'agl', 'agm', 'agn', 'ago', 'agp', 'agq', 'agr', 'ags', 'agt', 'agu', 'agv', 'agw', 'agx', 'agy', 'ahf', 'ahg', 'ahh', 'ahi', 'ahj', 'ahk', 'ahl', 'ahm', 'ahn', 'aho']\n",
    "features_1p4_extra =['aab', 'aac', 'aad', 'aae', 'aaf', 'aag', 'aah', 'abj', 'abm', 'abn', 'abo', 'abp', 'abq', 'abr', 'abs', 'abt', 'abu', 'abv', 'abw', 'abx', 'aby', 'abz', 'aca', 'acb', 'acc', 'acd', 'ace', 'acf', 'acr', 'acw', 'acx', 'acy', 'adi', 'adj', 'adl', 'ado', 'adp', 'adq', 'adr', 'ads', 'adt', 'adu', 'adv', 'adw', 'adx', 'ady', 'adz', 'aea', 'aeb', 'aec', 'aed', 'aee', 'aef', 'aeg', 'aeh', 'aei', 'aej', 'aek', 'ael', 'aem', 'aen', 'aeo', 'aep', 'aeq', 'aer', 'aes', 'aex', 'aey', 'aez', 'afa', 'afj', 'afl', 'afo', 'afp', 'afq', 'afr', 'afs', 'aft', 'afu', 'afv', 'afw', 'afx', 'afy', 'afz', 'aga', 'agb', 'agc', 'agd', 'age', 'agf', 'agg', 'agh', 'agi', 'agj', 'agk', 'agl', 'agm', 'agn', 'ago', 'agp', 'agq', 'agr', 'ags', 'agt', 'agu', 'agv', 'agw', 'agx', 'agy', 'ahf', 'ahg', 'ahh', 'ahi', 'ahj', 'ahk', 'ahl', 'ahm', 'ahn', 'aho', 'zhq', 'zhr', 'zhs', 'zht', 'zhu', 'zhv', 'zhw', 'ziy', 'zjb', 'zjc', 'zjd', 'zje', 'zjf', 'zjg', 'zjh', 'zji', 'zjj', 'zjk', 'zjl', 'zjm', 'zjn', 'zjo', 'zjp', 'zjq', 'zjr', 'zjs', 'zjt', 'zju', 'zkg', 'zkl', 'zkm', 'zkn', 'zkx', 'zky', 'zla', 'zld', 'zle', 'zlf', 'zlg', 'zlh', 'zli', 'zlj', 'zlk', 'zll', 'zlm', 'zln', 'zlo', 'zlp', 'zlq', 'zlr', 'zls', 'zlt', 'zlu', 'zlv', 'zlw', 'zlx', 'zly', 'zlz', 'zma', 'zmb', 'zmc', 'zmd', 'zme', 'zmf', 'zmg', 'zmh', 'zmm', 'zmn', 'zmo', 'zmp', 'zmy', 'zna', 'znd', 'zne', 'znf', 'zng', 'znh', 'zni', 'znj', 'znk', 'znl', 'znm', 'znn', 'zno', 'znp', 'znq', 'znr', 'zns', 'znt', 'znu', 'znv', 'znw', 'znx', 'zny', 'znz', 'zoa', 'zob', 'zoc', 'zod', 'zoe', 'zof', 'zog', 'zoh', 'zoi', 'zoj', 'zok', 'zol', 'zom', 'zon', 'zou', 'zov', 'zow', 'zox', 'zoy', 'zoz', 'zpa', 'zpb', 'zpc', 'zpd', 'zpe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "Cs = np.logspace(-4, 5)\n",
    "cv = 5\n",
    "ratio_threshold = 0.65\n",
    "date_test_set = datetime.date(2016, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data and clean with all possible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNB30 (8010, 239)\n",
      "NRG30 (5368, 239)\n",
      "CL30 (5187, 239)\n",
      "ANTM30 (2420, 239)\n",
      "NEE30 (4769, 239)\n",
      "PAYX30 (5537, 239)\n",
      "VAR30 (6070, 239)\n",
      "NI30 (4998, 239)\n",
      "MNST30 (6570, 239)\n",
      "JNJ30 (5791, 239)\n",
      "TGNA30 (1787, 239)\n",
      "NOV30 (5057, 239)\n",
      "FIS30 (6086, 239)\n",
      "BLK30 (5580, 239)\n",
      "HBI30 (6188, 239)\n",
      "NVDA30 (5295, 239)\n",
      "DLTR30 (5946, 239)\n",
      "MRO30 (4767, 239)\n",
      "EMN30 (5274, 239)\n",
      "AMT30 (5246, 239)\n",
      "FLR30 (5514, 239)\n",
      "IBM30 (5742, 239)\n",
      "BK30 (4715, 239)\n",
      "NFX30 (5060, 239)\n",
      "AGN30 (5383, 239)\n",
      "LRCX30 (5568, 239)\n",
      "DIS30 (5515, 239)\n",
      "LH30 (6348, 239)\n",
      "C30 (5429, 239)\n",
      "MNK30 (4938, 239)\n"
     ]
    }
   ],
   "source": [
    "clf_portfolio_dic = methodsMLinterns.ClassificationPortfolio(stocks=stocks, minutes_forward=30)\n",
    "clf_portfolio_dic.loadData()\n",
    "clf_portfolio_dic.cleanUpData(features_1p4_extra)\n",
    "clf_portfolio_dic.getTrainTestSetDate(date_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_1p4_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(features):\n",
    "    \n",
    "    x_train = np.array([], dtype=np.float32).reshape(0,len(features))\n",
    "    y_train = np.array([], dtype=np.float32).reshape(0,1)\n",
    "    x_test = np.array([], dtype=np.float32).reshape(0,len(features))\n",
    "    y_test = np.array([], dtype=np.float32).reshape(0,1)\n",
    "    \n",
    "    for k, stock in enumerate(clf_portfolio_dic.stocks):\n",
    "        name = stock + str(clf_portfolio_dic.minutes_forward)\n",
    "        if k==0:\n",
    "            x_train, y_train = clf_portfolio_dic.X_train_dic[name][features].as_matrix(),(clf_portfolio_dic.y_train_dic[name]+1)/2\n",
    "            x_test, y_test = clf_portfolio_dic.X_test_dic[name][features].as_matrix(), (clf_portfolio_dic.y_test_dic[name]+1)/2\n",
    "        else:\n",
    "            x_train = np.concatenate((x_train,clf_portfolio_dic.X_train_dic[name][features].as_matrix()),axis=0)\n",
    "            y_train = np.concatenate((y_train,(clf_portfolio_dic.y_train_dic[name]+1)/2),axis=0)\n",
    "            x_test = np.concatenate((x_test,clf_portfolio_dic.X_test_dic[name][features].as_matrix()),axis=0)\n",
    "            y_test = np.concatenate((y_test,(clf_portfolio_dic.y_test_dic[name]+1)/2),axis=0)\n",
    "\n",
    "    y_t = np.zeros((y_train.shape[0], 2))\n",
    "    y_t[np.arange(y_train.shape[0]), y_train.astype('int32')] = 1\n",
    "    y_train = y_t\n",
    "    \n",
    "    y_t = np.zeros((y_test.shape[0], 2))\n",
    "    y_t[np.arange(y_test.shape[0]), y_test.astype('int32')] = 1\n",
    "    y_val = y_t\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122607, 2)\n",
      "(37386,)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,X_test,Y_test = prepareData(features_1p4_extra)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18711912097\n"
     ]
    }
   ],
   "source": [
    "n=int(0.8*122607)\n",
    "pca = PCA(n_components=30, random_state=0)\n",
    "pca.fit(X_train[:n])\n",
    "X_test_encoded_pca = pca.transform(X_test)\n",
    "X_test_decoded_pca = pca.inverse_transform(pca.transform(X_test))\n",
    "pca_reconstruction_mse = np.mean(np.square(X_test_decoded_pca - X_test))\n",
    "print(pca_reconstruction_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "import keras.callbacks\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_name = datetime.datetime.today().strftime('%Y-%m-%d_%H:%M')\n",
    "monit = 'val_loss'\n",
    "filepath=\"%s/%s.hdf5\"%(\"weights_comparison\", log_name)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='/Users/clement/cqc/log/%s'%log_name\n",
    "                                      , histogram_freq=0, write_graph=True, write_images=True)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor=monit, verbose=1, save_best_only=True, mode='min')\n",
    "        \n",
    "# Add early stopping\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor=monit, min_delta=0, patience=300, verbose=1, mode='auto')\n",
    "\n",
    "cbs = [tensorboard, checkpoint, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildAutoencoder():\n",
    "    # size of encoded representations\n",
    "    encoding_dim = 30\n",
    "\n",
    "    input_img = Input(shape=(239,))\n",
    "    #encoded = Dense(enc, activation='relu')(input_img)\n",
    "    #encoded = Dropout(0.2)(encoded)\n",
    "    #encoded = Dense(64, activation='relu')(encoded)\n",
    "    #encoded = Dropout(0.2)(encoded)\n",
    "    #encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "        \n",
    "    encoded = Dense(100, activation='relu')(input_img)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    decoded = Dense(100, activation='relu')(encoded)\n",
    "    decoded = Dense(239)(decoded)\n",
    "\n",
    "    #decoded = Dense(64, activation='relu')(encoded)\n",
    "    #decoded = Dropout(0.1)(decoded)\n",
    "    #decoded = Dense(128, activation='relu')(decoded)\n",
    "    #decoded = Dropout(0.1)(decoded)\n",
    "    #decoded = Dense(239, activation='relu')(decoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input=input_img, output=decoded)\n",
    "\n",
    "    # this model maps an input to its encoded representation\n",
    "    encoder = Model(input=input_img, output=encoded)\n",
    "\n",
    "    # placeholder for encoded (32-dimensional) input\n",
    "    #encoded_input = Input(shape=(encoding_dim,))\n",
    "    # retrieve the last layer of the autoencoder model\n",
    "    #decoder_layer = autoencoder.layers[-3]\n",
    "    #decoder_layer1 = autoencoder.layers[-2]\n",
    "    #decoder_layer2 = autoencoder.layers[-1]\n",
    "\n",
    "    # create the decoder model\n",
    "    #decoder = Model(input=encoded_input, output=decoder_layer2(decoder_layer1(decoder_layer(encoded_input))))\n",
    "\n",
    "\n",
    "    ## Now let's train our autoencoder to reconstruct MNIST digits.\n",
    "    ## First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:\n",
    "    autoencoder.compile(optimizer='adadelta', loss='mse')\n",
    "    \n",
    "    return encoder,autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98085 samples, validate on 24522 samples\n",
      "Epoch 1/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.5400Epoch 00000: val_loss improved from inf to 0.33467, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.5393 - val_loss: 0.3347\n",
      "Epoch 2/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.2746Epoch 00001: val_loss improved from 0.33467 to 0.24790, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.2742 - val_loss: 0.2479\n",
      "Epoch 3/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.2236Epoch 00002: val_loss improved from 0.24790 to 0.21461, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.2236 - val_loss: 0.2146\n",
      "Epoch 4/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.2039Epoch 00003: val_loss improved from 0.21461 to 0.20156, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.2037 - val_loss: 0.2016\n",
      "Epoch 5/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1926Epoch 00004: val_loss improved from 0.20156 to 0.19267, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1926 - val_loss: 0.1927\n",
      "Epoch 6/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1844Epoch 00005: val_loss improved from 0.19267 to 0.18625, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1845 - val_loss: 0.1862\n",
      "Epoch 7/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1782Epoch 00006: val_loss improved from 0.18625 to 0.18171, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1782 - val_loss: 0.1817\n",
      "Epoch 8/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1737Epoch 00007: val_loss improved from 0.18171 to 0.17833, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1736 - val_loss: 0.1783\n",
      "Epoch 9/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1692Epoch 00008: val_loss improved from 0.17833 to 0.17402, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1690 - val_loss: 0.1740\n",
      "Epoch 10/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1653Epoch 00009: val_loss improved from 0.17402 to 0.17020, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1654 - val_loss: 0.1702\n",
      "Epoch 11/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1619Epoch 00010: val_loss improved from 0.17020 to 0.16542, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1619 - val_loss: 0.1654\n",
      "Epoch 12/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1598Epoch 00011: val_loss improved from 0.16542 to 0.16512, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1597 - val_loss: 0.1651\n",
      "Epoch 13/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1573Epoch 00012: val_loss improved from 0.16512 to 0.16041, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1573 - val_loss: 0.1604\n",
      "Epoch 14/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1555Epoch 00013: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1554 - val_loss: 0.1608\n",
      "Epoch 15/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1537Epoch 00014: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1537 - val_loss: 0.1629\n",
      "Epoch 16/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1520Epoch 00015: val_loss improved from 0.16041 to 0.15744, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1518 - val_loss: 0.1574\n",
      "Epoch 17/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1509Epoch 00016: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1508 - val_loss: 0.1637\n",
      "Epoch 18/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1499Epoch 00017: val_loss improved from 0.15744 to 0.15373, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1498 - val_loss: 0.1537\n",
      "Epoch 19/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1486Epoch 00018: val_loss improved from 0.15373 to 0.15260, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1485 - val_loss: 0.1526\n",
      "Epoch 20/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1478Epoch 00019: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1478 - val_loss: 0.1569\n",
      "Epoch 21/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1464Epoch 00020: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1464 - val_loss: 0.1572\n",
      "Epoch 22/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1461Epoch 00021: val_loss improved from 0.15260 to 0.15067, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1461 - val_loss: 0.1507\n",
      "Epoch 23/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1450Epoch 00022: val_loss improved from 0.15067 to 0.14843, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1450 - val_loss: 0.1484\n",
      "Epoch 24/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1443Epoch 00023: val_loss improved from 0.14843 to 0.14830, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1443 - val_loss: 0.1483\n",
      "Epoch 25/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1437Epoch 00024: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1437 - val_loss: 0.1497\n",
      "Epoch 26/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1440Epoch 00025: val_loss improved from 0.14830 to 0.14680, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1440 - val_loss: 0.1468\n",
      "Epoch 27/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1423Epoch 00026: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1424 - val_loss: 0.1524\n",
      "Epoch 28/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1428Epoch 00027: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1428 - val_loss: 0.1492\n",
      "Epoch 29/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1419Epoch 00028: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1419 - val_loss: 0.1478\n",
      "Epoch 30/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1425Epoch 00029: val_loss improved from 0.14680 to 0.14607, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1423 - val_loss: 0.1461\n",
      "Epoch 31/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1409Epoch 00030: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1411 - val_loss: 0.1487\n",
      "Epoch 32/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1408Epoch 00031: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1408 - val_loss: 0.1470\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1405Epoch 00032: val_loss improved from 0.14607 to 0.14398, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1405 - val_loss: 0.1440\n",
      "Epoch 34/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1402Epoch 00033: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1402 - val_loss: 0.1452\n",
      "Epoch 35/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1393Epoch 00034: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1394 - val_loss: 0.1443\n",
      "Epoch 36/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1394Epoch 00035: val_loss improved from 0.14398 to 0.14229, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1393 - val_loss: 0.1423\n",
      "Epoch 37/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1385Epoch 00036: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1385 - val_loss: 0.1534\n",
      "Epoch 38/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1381Epoch 00037: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1380 - val_loss: 0.1423\n",
      "Epoch 39/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1379Epoch 00038: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1379 - val_loss: 0.1459\n",
      "Epoch 40/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1367Epoch 00039: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1374 - val_loss: 0.1980\n",
      "Epoch 41/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1373Epoch 00040: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1373 - val_loss: 0.1429\n",
      "Epoch 42/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1365Epoch 00041: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1364 - val_loss: 0.1462\n",
      "Epoch 43/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1371Epoch 00042: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1371 - val_loss: 0.1424\n",
      "Epoch 44/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1360Epoch 00043: val_loss improved from 0.14229 to 0.14078, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1360 - val_loss: 0.1408\n",
      "Epoch 45/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1357Epoch 00044: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1357 - val_loss: 0.1499\n",
      "Epoch 46/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1354Epoch 00045: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1354 - val_loss: 0.1492\n",
      "Epoch 47/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1354Epoch 00046: val_loss improved from 0.14078 to 0.13765, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1352 - val_loss: 0.1377\n",
      "Epoch 48/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1341Epoch 00047: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1341 - val_loss: 0.1398\n",
      "Epoch 49/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1342Epoch 00048: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1342 - val_loss: 0.1379\n",
      "Epoch 50/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1338Epoch 00049: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1337 - val_loss: 0.1396\n",
      "Epoch 51/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1337Epoch 00050: val_loss improved from 0.13765 to 0.13694, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1337 - val_loss: 0.1369\n",
      "Epoch 52/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1328Epoch 00051: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1328 - val_loss: 0.1411\n",
      "Epoch 53/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1324Epoch 00052: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1324 - val_loss: 0.1406\n",
      "Epoch 54/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1325Epoch 00053: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1323 - val_loss: 0.1399\n",
      "Epoch 55/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1317Epoch 00054: val_loss improved from 0.13694 to 0.13503, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1316 - val_loss: 0.1350\n",
      "Epoch 56/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1315Epoch 00055: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1314 - val_loss: 0.1381\n",
      "Epoch 57/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1305Epoch 00056: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1305 - val_loss: 0.1390\n",
      "Epoch 58/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1301Epoch 00057: val_loss improved from 0.13503 to 0.13464, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1300 - val_loss: 0.1346\n",
      "Epoch 59/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1292Epoch 00058: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1292 - val_loss: 0.1363\n",
      "Epoch 60/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1287Epoch 00059: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1287 - val_loss: 0.1407\n",
      "Epoch 61/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1281Epoch 00060: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1282 - val_loss: 0.1468\n",
      "Epoch 62/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1276Epoch 00061: val_loss improved from 0.13464 to 0.13267, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1275 - val_loss: 0.1327\n",
      "Epoch 63/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1275Epoch 00062: val_loss improved from 0.13267 to 0.13081, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1275 - val_loss: 0.1308\n",
      "Epoch 64/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1267Epoch 00063: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1267 - val_loss: 0.1345\n",
      "Epoch 65/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1264Epoch 00064: val_loss improved from 0.13081 to 0.12960, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1264 - val_loss: 0.1296\n",
      "Epoch 66/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1256Epoch 00065: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1258 - val_loss: 0.1485\n",
      "Epoch 67/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1254Epoch 00066: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1253 - val_loss: 0.1341\n",
      "Epoch 68/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1249Epoch 00067: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1248 - val_loss: 0.1306\n",
      "Epoch 69/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1246Epoch 00068: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98085/98085 [==============================] - 3s - loss: 0.1247 - val_loss: 0.1448\n",
      "Epoch 70/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1246Epoch 00069: val_loss improved from 0.12960 to 0.12876, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1245 - val_loss: 0.1288\n",
      "Epoch 71/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1241Epoch 00070: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1241 - val_loss: 0.1288\n",
      "Epoch 72/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1237Epoch 00071: val_loss improved from 0.12876 to 0.12808, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1236 - val_loss: 0.1281\n",
      "Epoch 73/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1232Epoch 00072: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1233 - val_loss: 0.1333\n",
      "Epoch 74/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1232Epoch 00073: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1233 - val_loss: 0.1400\n",
      "Epoch 75/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1226Epoch 00074: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1226 - val_loss: 0.1321\n",
      "Epoch 76/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1226Epoch 00075: val_loss improved from 0.12808 to 0.12647, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1225 - val_loss: 0.1265\n",
      "Epoch 77/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1225Epoch 00076: val_loss improved from 0.12647 to 0.12508, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1225 - val_loss: 0.1251\n",
      "Epoch 78/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1221Epoch 00077: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1221 - val_loss: 0.1271\n",
      "Epoch 79/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1219Epoch 00078: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1218 - val_loss: 0.1251\n",
      "Epoch 80/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1219Epoch 00079: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1219 - val_loss: 0.1291\n",
      "Epoch 81/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1215Epoch 00080: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1214 - val_loss: 0.1257\n",
      "Epoch 82/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1216Epoch 00081: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1214 - val_loss: 0.1267\n",
      "Epoch 83/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1207Epoch 00082: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1207 - val_loss: 0.1272\n",
      "Epoch 84/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1206Epoch 00083: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1206 - val_loss: 0.1292\n",
      "Epoch 85/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1204Epoch 00084: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1204 - val_loss: 0.1364\n",
      "Epoch 86/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1204Epoch 00085: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1204 - val_loss: 0.1297\n",
      "Epoch 87/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1202Epoch 00086: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1202 - val_loss: 0.1261\n",
      "Epoch 88/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1201Epoch 00087: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1200 - val_loss: 0.1259\n",
      "Epoch 89/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1203Epoch 00088: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1203 - val_loss: 0.1330\n",
      "Epoch 90/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1200Epoch 00089: val_loss improved from 0.12508 to 0.12355, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1199 - val_loss: 0.1236\n",
      "Epoch 91/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1197Epoch 00090: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1197 - val_loss: 0.1260\n",
      "Epoch 92/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1194Epoch 00091: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1193 - val_loss: 0.1245\n",
      "Epoch 93/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1195Epoch 00092: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1194 - val_loss: 0.1251\n",
      "Epoch 94/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1191Epoch 00093: val_loss improved from 0.12355 to 0.12327, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1191 - val_loss: 0.1233\n",
      "Epoch 95/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1186Epoch 00094: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1191 - val_loss: 0.1425\n",
      "Epoch 96/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1182Epoch 00095: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1182 - val_loss: 0.1303\n",
      "Epoch 97/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1186Epoch 00096: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1186 - val_loss: 0.1248\n",
      "Epoch 98/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1186Epoch 00097: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1186 - val_loss: 0.1255\n",
      "Epoch 99/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1182Epoch 00098: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1181 - val_loss: 0.1269\n",
      "Epoch 100/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1179Epoch 00099: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1179 - val_loss: 0.1255\n",
      "Epoch 101/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1178Epoch 00100: val_loss improved from 0.12327 to 0.12277, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1178 - val_loss: 0.1228\n",
      "Epoch 102/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1175Epoch 00101: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1176 - val_loss: 0.1238\n",
      "Epoch 103/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1174Epoch 00102: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1173 - val_loss: 0.1237\n",
      "Epoch 104/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1172Epoch 00103: val_loss improved from 0.12277 to 0.12031, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1172 - val_loss: 0.1203\n",
      "Epoch 105/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1169Epoch 00104: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1169 - val_loss: 0.1223\n",
      "Epoch 106/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1168Epoch 00105: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98085/98085 [==============================] - 3s - loss: 0.1168 - val_loss: 0.1206\n",
      "Epoch 107/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1164Epoch 00106: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1164 - val_loss: 0.1232\n",
      "Epoch 108/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1166Epoch 00107: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1166 - val_loss: 0.1220\n",
      "Epoch 109/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1166Epoch 00108: val_loss improved from 0.12031 to 0.11984, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1165 - val_loss: 0.1198\n",
      "Epoch 110/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1161Epoch 00109: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1161 - val_loss: 0.1238\n",
      "Epoch 111/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1160Epoch 00110: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1160 - val_loss: 0.1214\n",
      "Epoch 112/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1157Epoch 00111: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1156 - val_loss: 0.1200\n",
      "Epoch 113/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1156Epoch 00112: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1155 - val_loss: 0.1204\n",
      "Epoch 114/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1154Epoch 00113: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1153 - val_loss: 0.1295\n",
      "Epoch 115/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1152Epoch 00114: val_loss improved from 0.11984 to 0.11956, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1152 - val_loss: 0.1196\n",
      "Epoch 116/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1151Epoch 00115: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1151 - val_loss: 0.1205\n",
      "Epoch 117/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1156Epoch 00116: val_loss improved from 0.11956 to 0.11880, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1156 - val_loss: 0.1188\n",
      "Epoch 118/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1147Epoch 00117: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1147 - val_loss: 0.1208\n",
      "Epoch 119/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1147Epoch 00118: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1146 - val_loss: 0.1202\n",
      "Epoch 120/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1146Epoch 00119: val_loss improved from 0.11880 to 0.11774, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1145 - val_loss: 0.1177\n",
      "Epoch 121/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1146Epoch 00120: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1146 - val_loss: 0.1201\n",
      "Epoch 122/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1141Epoch 00121: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1141 - val_loss: 0.1195\n",
      "Epoch 123/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1139Epoch 00122: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1139 - val_loss: 0.1189\n",
      "Epoch 124/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1137Epoch 00123: val_loss improved from 0.11774 to 0.11693, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1137 - val_loss: 0.1169\n",
      "Epoch 125/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1139Epoch 00124: val_loss improved from 0.11693 to 0.11636, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1139 - val_loss: 0.1164\n",
      "Epoch 126/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1135Epoch 00125: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1135 - val_loss: 0.1189\n",
      "Epoch 127/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1130Epoch 00126: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1130 - val_loss: 0.1192\n",
      "Epoch 128/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1130Epoch 00127: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1129 - val_loss: 0.1290\n",
      "Epoch 129/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1128Epoch 00128: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1128 - val_loss: 0.1314\n",
      "Epoch 130/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1126Epoch 00129: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1126 - val_loss: 0.1164\n",
      "Epoch 131/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1126Epoch 00130: val_loss improved from 0.11636 to 0.11578, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1125 - val_loss: 0.1158\n",
      "Epoch 132/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1126Epoch 00131: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1126 - val_loss: 0.1175\n",
      "Epoch 133/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1121Epoch 00132: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1121 - val_loss: 0.1212\n",
      "Epoch 134/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1121Epoch 00133: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1120 - val_loss: 0.1163\n",
      "Epoch 135/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1119Epoch 00134: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1120 - val_loss: 0.1184\n",
      "Epoch 136/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1115Epoch 00135: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1115 - val_loss: 0.1159\n",
      "Epoch 137/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1111Epoch 00136: val_loss improved from 0.11578 to 0.11578, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1111 - val_loss: 0.1158\n",
      "Epoch 138/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1110Epoch 00137: val_loss improved from 0.11578 to 0.11433, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1110 - val_loss: 0.1143\n",
      "Epoch 139/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1108Epoch 00138: val_loss improved from 0.11433 to 0.11367, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1108 - val_loss: 0.1137\n",
      "Epoch 140/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1109Epoch 00139: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1109 - val_loss: 0.1149\n",
      "Epoch 141/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1104Epoch 00140: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1104 - val_loss: 0.1159\n",
      "Epoch 142/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1100Epoch 00141: val_loss improved from 0.11367 to 0.11279, saving model to weights_comparison/2017-07-25_15:36.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98085/98085 [==============================] - 3s - loss: 0.1100 - val_loss: 0.1128\n",
      "Epoch 143/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1103Epoch 00142: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1104 - val_loss: 0.1151\n",
      "Epoch 144/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1095Epoch 00143: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1095 - val_loss: 0.1140\n",
      "Epoch 145/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1096Epoch 00144: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1095 - val_loss: 0.1129\n",
      "Epoch 146/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1095Epoch 00145: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1096 - val_loss: 0.1152\n",
      "Epoch 147/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1092Epoch 00146: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1091 - val_loss: 0.1133\n",
      "Epoch 148/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1090Epoch 00147: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1091 - val_loss: 0.1199\n",
      "Epoch 149/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1089Epoch 00148: val_loss improved from 0.11279 to 0.11233, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1089 - val_loss: 0.1123\n",
      "Epoch 150/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1091Epoch 00149: val_loss improved from 0.11233 to 0.11108, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1091 - val_loss: 0.1111\n",
      "Epoch 151/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1087Epoch 00150: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1087 - val_loss: 0.1147\n",
      "Epoch 152/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1082Epoch 00151: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1083 - val_loss: 0.1169\n",
      "Epoch 153/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1086Epoch 00152: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1085 - val_loss: 0.1126\n",
      "Epoch 154/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1081Epoch 00153: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1081 - val_loss: 0.1154\n",
      "Epoch 155/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1080Epoch 00154: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1079 - val_loss: 0.1145\n",
      "Epoch 156/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1076Epoch 00155: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1076 - val_loss: 0.1145\n",
      "Epoch 157/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1076Epoch 00156: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1077 - val_loss: 0.1182\n",
      "Epoch 158/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1072Epoch 00157: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1078 - val_loss: 0.1126\n",
      "Epoch 159/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1070Epoch 00158: val_loss improved from 0.11108 to 0.11072, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1070 - val_loss: 0.1107\n",
      "Epoch 160/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1071Epoch 00159: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1071 - val_loss: 0.1130\n",
      "Epoch 161/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1070Epoch 00160: val_loss improved from 0.11072 to 0.11030, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1070 - val_loss: 0.1103\n",
      "Epoch 162/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1071Epoch 00161: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1072 - val_loss: 0.1137\n",
      "Epoch 163/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1068Epoch 00162: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1069 - val_loss: 0.1138\n",
      "Epoch 164/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1065Epoch 00163: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1065 - val_loss: 0.1131\n",
      "Epoch 165/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1066Epoch 00164: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1066 - val_loss: 0.1109\n",
      "Epoch 166/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1067Epoch 00165: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1067 - val_loss: 0.1161\n",
      "Epoch 167/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1064Epoch 00166: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1064 - val_loss: 0.1107\n",
      "Epoch 168/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1059Epoch 00167: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1064 - val_loss: 0.1131\n",
      "Epoch 169/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1058Epoch 00168: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1058 - val_loss: 0.1123\n",
      "Epoch 170/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1062Epoch 00169: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1062 - val_loss: 0.1302\n",
      "Epoch 171/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1059Epoch 00170: val_loss improved from 0.11030 to 0.11012, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1059 - val_loss: 0.1101\n",
      "Epoch 172/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1060Epoch 00171: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1059 - val_loss: 0.1202\n",
      "Epoch 173/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1057Epoch 00172: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1057 - val_loss: 0.1107\n",
      "Epoch 174/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1056Epoch 00173: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1056 - val_loss: 0.1143\n",
      "Epoch 175/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1052Epoch 00174: val_loss improved from 0.11012 to 0.10978, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1053 - val_loss: 0.1098\n",
      "Epoch 176/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1058Epoch 00175: val_loss improved from 0.10978 to 0.10829, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1057 - val_loss: 0.1083\n",
      "Epoch 177/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1054Epoch 00176: val_loss improved from 0.10829 to 0.10776, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 3s - loss: 0.1054 - val_loss: 0.1078\n",
      "Epoch 178/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1052Epoch 00177: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1052 - val_loss: 0.1113\n",
      "Epoch 179/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1052Epoch 00178: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98085/98085 [==============================] - 3s - loss: 0.1052 - val_loss: 0.1129\n",
      "Epoch 180/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1051Epoch 00179: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1051 - val_loss: 0.1138\n",
      "Epoch 181/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1046Epoch 00180: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1046 - val_loss: 0.1091\n",
      "Epoch 182/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1048Epoch 00181: val_loss improved from 0.10776 to 0.10719, saving model to weights_comparison/2017-07-25_15:36.hdf5\n",
      "98085/98085 [==============================] - 4s - loss: 0.1048 - val_loss: 0.1072\n",
      "Epoch 183/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1048Epoch 00182: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1048 - val_loss: 0.1150\n",
      "Epoch 184/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1047Epoch 00183: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1047 - val_loss: 0.1084\n",
      "Epoch 185/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1046Epoch 00184: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1045 - val_loss: 0.1102\n",
      "Epoch 186/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1046Epoch 00185: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1045 - val_loss: 0.1096\n",
      "Epoch 187/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1044Epoch 00186: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1044 - val_loss: 0.1113\n",
      "Epoch 188/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1040Epoch 00187: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1040 - val_loss: 0.1157\n",
      "Epoch 189/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1039Epoch 00188: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1040 - val_loss: 0.1156\n",
      "Epoch 190/200\n",
      "97280/98085 [============================>.] - ETA: 0s - loss: 0.1041Epoch 00189: val_loss did not improve\n",
      "98085/98085 [==============================] - 3s - loss: 0.1041 - val_loss: 0.1125\n",
      "Epoch 191/200\n",
      "96768/98085 [============================>.] - ETA: 0s - loss: 0.1041Epoch 00190: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1040 - val_loss: 0.1080\n",
      "Epoch 192/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1038Epoch 00191: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1037 - val_loss: 0.1116\n",
      "Epoch 193/200\n",
      "97024/98085 [============================>.] - ETA: 0s - loss: 0.1040Epoch 00192: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1039 - val_loss: 0.1075\n",
      "Epoch 194/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1040Epoch 00193: val_loss did not improve\n",
      "98085/98085 [==============================] - 5s - loss: 0.1041 - val_loss: 0.1077\n",
      "Epoch 195/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1035Epoch 00194: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1035 - val_loss: 0.1088\n",
      "Epoch 196/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1038Epoch 00195: val_loss did not improve\n",
      "98085/98085 [==============================] - 5s - loss: 0.1037 - val_loss: 0.1081\n",
      "Epoch 197/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1032Epoch 00196: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1031 - val_loss: 0.1079\n",
      "Epoch 198/200\n",
      "97792/98085 [============================>.] - ETA: 0s - loss: 0.1035Epoch 00197: val_loss did not improve\n",
      "98085/98085 [==============================] - 5s - loss: 0.1035 - val_loss: 0.1073\n",
      "Epoch 199/200\n",
      "98048/98085 [============================>.] - ETA: 0s - loss: 0.1032Epoch 00198: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1032 - val_loss: 0.1105\n",
      "Epoch 200/200\n",
      "97536/98085 [============================>.] - ETA: 0s - loss: 0.1030Epoch 00199: val_loss did not improve\n",
      "98085/98085 [==============================] - 4s - loss: 0.1030 - val_loss: 0.1104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14da168d0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder,autoencoder = buildAutoencoder()\n",
    "\n",
    "autoencoder.fit(X_train, X_train,\n",
    "               nb_epoch=200,\n",
    "               batch_size=256,\n",
    "               shuffle=True,\n",
    "               validation_split=0.2,\n",
    "               verbose=1,\n",
    "                callbacks=cbs\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.187974392451\n",
      "0.188748371987\n",
      "0.187958273248\n",
      "0.190255996742\n",
      "0.132231063095\n",
      "0.131395794387\n",
      "0.188130139248\n",
      "0.162577768631\n"
     ]
    }
   ],
   "source": [
    "autoencoder.load_weights(filepath)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "decoded_test = autoencoder.predict(X_test)\n",
    "auto_reconstruction_mse_8 = np.mean(np.square(decoded_test - X_test))\n",
    "\n",
    "print(auto_reconstruction_mse_1)\n",
    "print(auto_reconstruction_mse_2)\n",
    "print(auto_reconstruction_mse_3)\n",
    "print(auto_reconstruction_mse_4)\n",
    "print(auto_reconstruction_mse_5)\n",
    "print(auto_reconstruction_mse_6)\n",
    "print(auto_reconstruction_mse_7)\n",
    "print(auto_reconstruction_mse_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment13 = methodsMLinterns.ExperimentPerformance(\n",
    "    methodName=\"Logistic Regression with Autoencoder (first try - 6 layers) - 1.4extra\",\n",
    "    stocks=stocks, originalFeatures=features_1p4_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy (mean - std) is:\n",
      "48.8035611463\n"
     ]
    }
   ],
   "source": [
    "experiment13.setTrainResults(acc_train_autoencoder)\n",
    "experiment13.setTestResults(acc_test_autoencoder)\n",
    "print(\"The test accuracy (mean - std) is:\")\n",
    "print(experiment13.getTestAccuracyMinusSigma())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"acc_autoencoder_32.p\",'wb') as f:\n",
    "    pickle.dump( experiment13, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean \t**   mean-std\n",
      "======== without filter ========\n",
      "51.9486374551 \t**  49.7480893744\n",
      "51.9486374551 \t**  49.7480893744\n",
      "51.7176323861 \t**  49.8987252822\n",
      "50.9361038116 \t**  49.0866498843\n",
      "=========== with PCA ===========\n",
      "53.0516027966 \t**  51.1807470076\n",
      "53.0516027966 \t**  51.1807470076\n",
      "53.1098755881 \t**  51.6694362323\n",
      "52.723259907 \t**  51.2981206182\n",
      "=========== with SVD ===========\n",
      "52.9917186486 \t**  51.0773956333\n",
      "52.9917186486 \t**  51.0773956333\n",
      "53.1047204198 \t**  51.6677227475\n",
      "52.7764900964 \t**  51.267525441\n",
      "===== with Autoencoder 32 ======\n",
      "50.5560683138 \t**  48.8035611463\n"
     ]
    }
   ],
   "source": [
    "print(\"   mean \\t**   mean-std\")\n",
    "print(\"======== without filter ========\")\n",
    "print(experiment1.getTestAccuracy(),\"\\t** \",experiment1.getTestAccuracyMinusSigma())\n",
    "print(experiment2.getTestAccuracy(),\"\\t** \",experiment2.getTestAccuracyMinusSigma())\n",
    "print(experiment3.getTestAccuracy(),\"\\t** \",experiment3.getTestAccuracyMinusSigma())\n",
    "print(experiment4.getTestAccuracy(),\"\\t** \",experiment4.getTestAccuracyMinusSigma())\n",
    "\n",
    "print(\"=========== with PCA ===========\")\n",
    "print(experiment5.getTestAccuracy(max=True),\"\\t** \",experiment5.getTestAccuracyMinusSigma(max=True))\n",
    "print(experiment6.getTestAccuracy(max=True),\"\\t** \",experiment6.getTestAccuracyMinusSigma(max=True))\n",
    "print(experiment7.getTestAccuracy(max=True),\"\\t** \",experiment7.getTestAccuracyMinusSigma(max=True))\n",
    "print(experiment8.getTestAccuracy(max=True),\"\\t** \",experiment8.getTestAccuracyMinusSigma(max=True))\n",
    "\n",
    "print(\"=========== with SVD ===========\")\n",
    "print(experiment9.getTestAccuracy(max=True),\"\\t** \",experiment9.getTestAccuracyMinusSigma(max=True))\n",
    "print(experiment10.getTestAccuracy(max=True),\"\\t** \",experiment10.getTestAccuracyMinusSigma(max=True))\n",
    "print(experiment11.getTestAccuracy(max=True),\"\\t** \",experiment11.getTestAccuracyMinusSigma(max=True))\n",
    "print(experiment12.getTestAccuracy(max=True),\"\\t** \",experiment12.getTestAccuracyMinusSigma(max=True))\n",
    "\n",
    "print(\"===== with Autoencoder 32 ======\")\n",
    "print(experiment13.getTestAccuracy(),\"\\t** \",experiment13.getTestAccuracyMinusSigma())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
