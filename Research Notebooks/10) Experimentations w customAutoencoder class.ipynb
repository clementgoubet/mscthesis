{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import methodsMLinterns\n",
    "from customAutoencoder import ModelType, Autoencoder, ExperimentPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = ['DNB', 'NRG', 'CL', 'ANTM', 'NEE', 'PAYX', 'VAR', 'NI', 'MNST', 'JNJ', 'TGNA', 'NOV', 'FIS', 'BLK', 'HBI', 'NVDA', 'DLTR', 'MRO', 'EMN', 'AMT', 'FLR', 'IBM', 'BK', 'NFX', 'AGN', 'LRCX', 'DIS', 'LH', 'C', 'MNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_1p4_extra =['aab', 'aac', 'aad', 'aae', 'aaf', 'aag', 'aah', 'abj', 'abm', 'abn', 'abo', 'abp', 'abq', 'abr', 'abs', 'abt', 'abu', 'abv', 'abw', 'abx', 'aby', 'abz', 'aca', 'acb', 'acc', 'acd', 'ace', 'acf', 'acr', 'acw', 'acx', 'acy', 'adi', 'adj', 'adl', 'ado', 'adp', 'adq', 'adr', 'ads', 'adt', 'adu', 'adv', 'adw', 'adx', 'ady', 'adz', 'aea', 'aeb', 'aec', 'aed', 'aee', 'aef', 'aeg', 'aeh', 'aei', 'aej', 'aek', 'ael', 'aem', 'aen', 'aeo', 'aep', 'aeq', 'aer', 'aes', 'aex', 'aey', 'aez', 'afa', 'afj', 'afl', 'afo', 'afp', 'afq', 'afr', 'afs', 'aft', 'afu', 'afv', 'afw', 'afx', 'afy', 'afz', 'aga', 'agb', 'agc', 'agd', 'age', 'agf', 'agg', 'agh', 'agi', 'agj', 'agk', 'agl', 'agm', 'agn', 'ago', 'agp', 'agq', 'agr', 'ags', 'agt', 'agu', 'agv', 'agw', 'agx', 'agy', 'ahf', 'ahg', 'ahh', 'ahi', 'ahj', 'ahk', 'ahl', 'ahm', 'ahn', 'aho', 'zhq', 'zhr', 'zhs', 'zht', 'zhu', 'zhv', 'zhw', 'ziy', 'zjb', 'zjc', 'zjd', 'zje', 'zjf', 'zjg', 'zjh', 'zji', 'zjj', 'zjk', 'zjl', 'zjm', 'zjn', 'zjo', 'zjp', 'zjq', 'zjr', 'zjs', 'zjt', 'zju', 'zkg', 'zkl', 'zkm', 'zkn', 'zkx', 'zky', 'zla', 'zld', 'zle', 'zlf', 'zlg', 'zlh', 'zli', 'zlj', 'zlk', 'zll', 'zlm', 'zln', 'zlo', 'zlp', 'zlq', 'zlr', 'zls', 'zlt', 'zlu', 'zlv', 'zlw', 'zlx', 'zly', 'zlz', 'zma', 'zmb', 'zmc', 'zmd', 'zme', 'zmf', 'zmg', 'zmh', 'zmm', 'zmn', 'zmo', 'zmp', 'zmy', 'zna', 'znd', 'zne', 'znf', 'zng', 'znh', 'zni', 'znj', 'znk', 'znl', 'znm', 'znn', 'zno', 'znp', 'znq', 'znr', 'zns', 'znt', 'znu', 'znv', 'znw', 'znx', 'zny', 'znz', 'zoa', 'zob', 'zoc', 'zod', 'zoe', 'zof', 'zog', 'zoh', 'zoi', 'zoj', 'zok', 'zol', 'zom', 'zon', 'zou', 'zov', 'zow', 'zox', 'zoy', 'zoz', 'zpa', 'zpb', 'zpc', 'zpd', 'zpe']\n",
    "features_1p4_extra_a=[f for f in features_1p4_extra if f[0]=='a']\n",
    "features_1p4_extra_z=[f for f in features_1p4_extra if f[0]=='z']\n",
    "assert(len(features_1p4_extra_a)+len(features_1p4_extra_z)==len(features_1p4_extra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_test_set = datetime.date(2016, 1, 1)\n",
    "\n",
    "clf_portfolio_dic = methodsMLinterns.ClassificationPortfolio(stocks=stocks, minutes_forward=30)\n",
    "clf_portfolio_dic.loadData()\n",
    "clf_portfolio_dic.cleanUpData(features_1p4_extra)\n",
    "clf_portfolio_dic.getTrainTestSetDate(date_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Group together all the stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(features):\n",
    "    X_train = np.array([], dtype=np.float64).reshape(0,len(features))\n",
    "    y_train = np.array([], dtype=np.float64).reshape(0,1)\n",
    "    X_test = np.array([], dtype=np.float64).reshape(0,len(features))\n",
    "    y_test = np.array([], dtype=np.float64).reshape(0,1)\n",
    "\n",
    "\n",
    "    for k, stock in enumerate(clf_portfolio_dic.stocks):\n",
    "        name = stock + str(clf_portfolio_dic.minutes_forward)\n",
    "        if k==0:\n",
    "            X_train, y_train = clf_portfolio_dic.X_train_dic[name][features].as_matrix(),(clf_portfolio_dic.y_train_dic[name]+1)/2\n",
    "            X_test, y_test = clf_portfolio_dic.X_test_dic[name][features].as_matrix(), (clf_portfolio_dic.y_test_dic[name]+1)/2\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train,clf_portfolio_dic.X_train_dic[name][features].as_matrix()),axis=0)\n",
    "            y_train = np.concatenate((y_train,(clf_portfolio_dic.y_train_dic[name]+1)/2),axis=0)\n",
    "            X_test = np.concatenate((X_test,clf_portfolio_dic.X_test_dic[name][features].as_matrix()),axis=0)\n",
    "            y_test = np.concatenate((y_test,(clf_portfolio_dic.y_test_dic[name]+1)/2),axis=0)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    # Transform to one hot vectors\n",
    "    y_t = np.zeros((y_train.shape[0], 2))\n",
    "    y_t[np.arange(y_train.shape[0]), y_train.astype('int32')] = 1\n",
    "    y_train = y_t\n",
    "\n",
    "    y_t = np.zeros((y_test.shape[0], 2))\n",
    "    y_t[np.arange(y_test.shape[0]), y_test.astype('int32')] = 1\n",
    "    y_test = y_t\n",
    "    \n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120, 100, 30]\n"
     ]
    }
   ],
   "source": [
    "modelType = ModelType.Merged\n",
    "\n",
    "# Define the NN architecture\n",
    "features = features_1p4_extra_z\n",
    "architecture = [len(features),100,30]\n",
    "print(architecture)\n",
    "\n",
    "listOfParameters = {}\n",
    "listOfParameters[\"z_30\"] = [np.asarray([0.00684]), [0.005], [256], [0.0698], ['cosine_proximity']]\n",
    "lp = listOfParameters[\"z_30\"]\n",
    "\n",
    "weightsDirectory=\"_weights\"\n",
    "\n",
    "with open(\"pickles/CV_setsIndexes.p\",'rb') as f:\n",
    "    sets = pickle.load(f)\n",
    "    \n",
    "train_idx = sets[0][0]\n",
    "val_idx = sets[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d_1': 0.06989977789379659, 'd': 0.0068486252678771065, 'autoencoderLoss': 0, 'l2': 0, 'batch_size': 1}\n"
     ]
    }
   ],
   "source": [
    "with open(\"pickles/CVhyperasSearchBestParameters_z_30.p\",'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122607, 120)\n",
      "\n",
      "\n",
      "\t First part:\n",
      "\n",
      "Epoch 00000: val_decoded_loss improved from inf to -0.00439, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00001: val_decoded_loss improved from -0.00439 to -0.00571, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00002: val_decoded_loss improved from -0.00571 to -0.00615, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00003: val_decoded_loss improved from -0.00615 to -0.00636, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00004: val_decoded_loss improved from -0.00636 to -0.00650, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00005: val_decoded_loss improved from -0.00650 to -0.00661, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00006: val_decoded_loss improved from -0.00661 to -0.00670, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00007: val_decoded_loss improved from -0.00670 to -0.00676, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00008: val_decoded_loss improved from -0.00676 to -0.00682, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00009: val_decoded_loss improved from -0.00682 to -0.00686, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00010: val_decoded_loss improved from -0.00686 to -0.00692, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00011: val_decoded_loss improved from -0.00692 to -0.00695, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00012: val_decoded_loss improved from -0.00695 to -0.00699, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00013: val_decoded_loss improved from -0.00699 to -0.00702, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00014: val_decoded_loss improved from -0.00702 to -0.00705, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00015: val_decoded_loss improved from -0.00705 to -0.00707, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00016: val_decoded_loss improved from -0.00707 to -0.00710, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00017: val_decoded_loss improved from -0.00710 to -0.00712, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00018: val_decoded_loss improved from -0.00712 to -0.00714, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00019: val_decoded_loss improved from -0.00714 to -0.00716, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00020: val_decoded_loss improved from -0.00716 to -0.00717, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00021: val_decoded_loss improved from -0.00717 to -0.00719, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00022: val_decoded_loss improved from -0.00719 to -0.00720, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00023: val_decoded_loss improved from -0.00720 to -0.00722, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00024: val_decoded_loss improved from -0.00722 to -0.00723, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00025: val_decoded_loss improved from -0.00723 to -0.00724, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00026: val_decoded_loss improved from -0.00724 to -0.00725, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00027: val_decoded_loss improved from -0.00725 to -0.00726, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00028: val_decoded_loss improved from -0.00726 to -0.00727, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00029: val_decoded_loss improved from -0.00727 to -0.00728, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00030: val_decoded_loss improved from -0.00728 to -0.00729, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00031: val_decoded_loss improved from -0.00729 to -0.00730, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00032: val_decoded_loss improved from -0.00730 to -0.00730, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00033: val_decoded_loss improved from -0.00730 to -0.00731, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00034: val_decoded_loss improved from -0.00731 to -0.00732, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00035: val_decoded_loss improved from -0.00732 to -0.00732, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00036: val_decoded_loss improved from -0.00732 to -0.00733, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00037: val_decoded_loss improved from -0.00733 to -0.00733, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00038: val_decoded_loss improved from -0.00733 to -0.00734, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00039: val_decoded_loss improved from -0.00734 to -0.00734, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00040: val_decoded_loss improved from -0.00734 to -0.00735, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00041: val_decoded_loss improved from -0.00735 to -0.00735, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00042: val_decoded_loss improved from -0.00735 to -0.00736, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00043: val_decoded_loss improved from -0.00736 to -0.00736, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00044: val_decoded_loss improved from -0.00736 to -0.00737, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00045: val_decoded_loss improved from -0.00737 to -0.00737, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00046: val_decoded_loss improved from -0.00737 to -0.00737, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00047: val_decoded_loss improved from -0.00737 to -0.00737, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00048: val_decoded_loss improved from -0.00737 to -0.00738, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00049: val_decoded_loss improved from -0.00738 to -0.00738, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00050: val_decoded_loss improved from -0.00738 to -0.00738, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00051: val_decoded_loss improved from -0.00738 to -0.00739, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00052: val_decoded_loss improved from -0.00739 to -0.00739, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00053: val_decoded_loss improved from -0.00739 to -0.00739, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00054: val_decoded_loss improved from -0.00739 to -0.00739, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00055: val_decoded_loss improved from -0.00739 to -0.00740, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00056: val_decoded_loss improved from -0.00740 to -0.00740, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00057: val_decoded_loss improved from -0.00740 to -0.00740, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00058: val_decoded_loss improved from -0.00740 to -0.00740, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00059: val_decoded_loss improved from -0.00740 to -0.00740, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00060: val_decoded_loss improved from -0.00740 to -0.00741, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00061: val_decoded_loss improved from -0.00741 to -0.00741, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00062: val_decoded_loss improved from -0.00741 to -0.00741, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00063: val_decoded_loss improved from -0.00741 to -0.00741, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00064: val_decoded_loss improved from -0.00741 to -0.00742, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00065: val_decoded_loss did not improve\n",
      "Epoch 00066: val_decoded_loss improved from -0.00742 to -0.00742, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00067: val_decoded_loss improved from -0.00742 to -0.00742, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00068: val_decoded_loss improved from -0.00742 to -0.00742, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00069: val_decoded_loss improved from -0.00742 to -0.00742, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00070: val_decoded_loss improved from -0.00742 to -0.00743, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00071: val_decoded_loss improved from -0.00743 to -0.00743, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00072: val_decoded_loss improved from -0.00743 to -0.00743, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00073: val_decoded_loss improved from -0.00743 to -0.00743, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00074: val_decoded_loss did not improve\n",
      "Epoch 00075: val_decoded_loss improved from -0.00743 to -0.00743, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00076: val_decoded_loss did not improve\n",
      "Epoch 00077: val_decoded_loss improved from -0.00743 to -0.00743, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00078: val_decoded_loss did not improve\n",
      "Epoch 00079: val_decoded_loss improved from -0.00743 to -0.00744, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00080: val_decoded_loss improved from -0.00744 to -0.00744, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00081: val_decoded_loss did not improve\n",
      "Epoch 00082: val_decoded_loss improved from -0.00744 to -0.00744, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00083: val_decoded_loss improved from -0.00744 to -0.00744, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00084: val_decoded_loss improved from -0.00744 to -0.00744, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00085: val_decoded_loss improved from -0.00744 to -0.00744, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00086: val_decoded_loss improved from -0.00744 to -0.00744, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00087: val_decoded_loss improved from -0.00744 to -0.00744, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00088: val_decoded_loss improved from -0.00744 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00089: val_decoded_loss did not improve\n",
      "Epoch 00090: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00091: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00092: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00093: val_decoded_loss did not improve\n",
      "Epoch 00094: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00095: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00096: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00097: val_decoded_loss did not improve\n",
      "Epoch 00098: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00099: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00100: val_decoded_loss did not improve\n",
      "Epoch 00101: val_decoded_loss did not improve\n",
      "Epoch 00102: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00103: val_decoded_loss improved from -0.00745 to -0.00745, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00104: val_decoded_loss did not improve\n",
      "Epoch 00105: val_decoded_loss improved from -0.00745 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00106: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00107: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00108: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00109: val_decoded_loss did not improve\n",
      "Epoch 00110: val_decoded_loss did not improve\n",
      "Epoch 00111: val_decoded_loss did not improve\n",
      "Epoch 00112: val_decoded_loss did not improve\n",
      "Epoch 00113: val_decoded_loss did not improve\n",
      "Epoch 00114: val_decoded_loss did not improve\n",
      "Epoch 00115: val_decoded_loss did not improve\n",
      "Epoch 00116: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00117: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00118: val_decoded_loss did not improve\n",
      "Epoch 00119: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00120: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00121: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00122: val_decoded_loss did not improve\n",
      "Epoch 00123: val_decoded_loss did not improve\n",
      "Epoch 00124: val_decoded_loss did not improve\n",
      "Epoch 00125: val_decoded_loss did not improve\n",
      "Epoch 00126: val_decoded_loss did not improve\n",
      "Epoch 00127: val_decoded_loss did not improve\n",
      "Epoch 00128: val_decoded_loss did not improve\n",
      "Epoch 00129: val_decoded_loss did not improve\n",
      "Epoch 00130: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00131: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00132: val_decoded_loss did not improve\n",
      "Epoch 00133: val_decoded_loss did not improve\n",
      "Epoch 00134: val_decoded_loss did not improve\n",
      "Epoch 00135: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00136: val_decoded_loss did not improve\n",
      "Epoch 00137: val_decoded_loss did not improve\n",
      "Epoch 00138: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00139: val_decoded_loss did not improve\n",
      "Epoch 00140: val_decoded_loss did not improve\n",
      "Epoch 00141: val_decoded_loss did not improve\n",
      "Epoch 00142: val_decoded_loss did not improve\n",
      "Epoch 00143: val_decoded_loss did not improve\n",
      "Epoch 00144: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00145: val_decoded_loss did not improve\n",
      "Epoch 00146: val_decoded_loss did not improve\n",
      "Epoch 00147: val_decoded_loss did not improve\n",
      "Epoch 00148: val_decoded_loss improved from -0.00746 to -0.00746, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00149: val_decoded_loss did not improve\n",
      "Epoch 00150: val_decoded_loss did not improve\n",
      "Epoch 00151: val_decoded_loss did not improve\n",
      "Epoch 00152: val_decoded_loss did not improve\n",
      "Epoch 00153: val_decoded_loss did not improve\n",
      "Epoch 00154: val_decoded_loss did not improve\n",
      "Epoch 00155: val_decoded_loss improved from -0.00746 to -0.00747, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00156: val_decoded_loss improved from -0.00747 to -0.00747, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00157: val_decoded_loss did not improve\n",
      "Epoch 00158: val_decoded_loss did not improve\n",
      "Epoch 00159: val_decoded_loss did not improve\n",
      "Epoch 00160: val_decoded_loss did not improve\n",
      "Epoch 00161: val_decoded_loss did not improve\n",
      "Epoch 00162: val_decoded_loss did not improve\n",
      "Epoch 00163: val_decoded_loss did not improve\n",
      "Epoch 00164: val_decoded_loss did not improve\n",
      "Epoch 00165: val_decoded_loss did not improve\n",
      "Epoch 00166: val_decoded_loss improved from -0.00747 to -0.00747, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00167: val_decoded_loss did not improve\n",
      "Epoch 00168: val_decoded_loss did not improve\n",
      "Epoch 00169: val_decoded_loss did not improve\n",
      "Epoch 00170: val_decoded_loss did not improve\n",
      "Epoch 00171: val_decoded_loss did not improve\n",
      "Epoch 00172: val_decoded_loss did not improve\n",
      "Epoch 00173: val_decoded_loss improved from -0.00747 to -0.00747, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00174: val_decoded_loss did not improve\n",
      "Epoch 00175: val_decoded_loss did not improve\n",
      "Epoch 00176: val_decoded_loss did not improve\n",
      "Epoch 00177: val_decoded_loss did not improve\n",
      "Epoch 00178: val_decoded_loss did not improve\n",
      "Epoch 00179: val_decoded_loss did not improve\n",
      "Epoch 00180: val_decoded_loss did not improve\n",
      "Epoch 00181: val_decoded_loss did not improve\n",
      "Epoch 00182: val_decoded_loss did not improve\n",
      "Epoch 00183: val_decoded_loss did not improve\n",
      "Epoch 00184: val_decoded_loss did not improve\n",
      "Epoch 00185: val_decoded_loss did not improve\n",
      "Epoch 00186: val_decoded_loss did not improve\n",
      "Epoch 00187: val_decoded_loss improved from -0.00747 to -0.00747, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "Epoch 00188: val_decoded_loss did not improve\n",
      "Epoch 00189: val_decoded_loss did not improve\n",
      "Epoch 00190: val_decoded_loss did not improve\n",
      "Epoch 00191: val_decoded_loss did not improve\n",
      "Epoch 00192: val_decoded_loss did not improve\n",
      "Epoch 00193: val_decoded_loss did not improve\n",
      "Epoch 00194: val_decoded_loss did not improve\n",
      "Epoch 00195: val_decoded_loss did not improve\n",
      "Epoch 00196: val_decoded_loss did not improve\n",
      "Epoch 00197: val_decoded_loss did not improve\n",
      "Epoch 00198: val_decoded_loss did not improve\n",
      "Epoch 00199: val_decoded_loss did not improve\n",
      "\n",
      "\n",
      "\t Second part:\n",
      "\n",
      "_weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 120)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 120)           0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 100)           12100       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 100)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "encoded (Dense)                  (None, 30)            3030        dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 100)           3100        encoded[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 100)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "decoded (Dense)                  (None, 120)           12120       dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "logit (Dense)                    (None, 2)             62          encoded[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 30,412\n",
      "Trainable params: 62\n",
      "Non-trainable params: 30,350\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 00000: val_logit_loss improved from inf to 0.66941, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f1.hdf5\n",
      "Epoch 00001: val_logit_loss improved from 0.66941 to 0.66924, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002: val_logit_loss improved from 0.66924 to 0.66922, saving model to _weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f1.hdf5\n",
      "Epoch 00003: val_logit_loss did not improve\n",
      "Epoch 00004: val_logit_loss did not improve\n",
      "Epoch 00005: val_logit_loss did not improve\n",
      "Epoch 00006: val_logit_loss did not improve\n",
      "Epoch 00007: val_logit_loss did not improve\n",
      "Epoch 00008: val_logit_loss did not improve\n",
      "Epoch 00009: val_logit_loss did not improve\n",
      "Epoch 00010: val_logit_loss did not improve\n",
      "Epoch 00011: val_logit_loss did not improve\n",
      "Epoch 00012: val_logit_loss did not improve\n",
      "Epoch 00013: val_logit_loss did not improve\n",
      "Epoch 00014: val_logit_loss did not improve\n",
      "Epoch 00015: val_logit_loss did not improve\n",
      "Epoch 00016: val_logit_loss did not improve\n",
      "Epoch 00017: val_logit_loss did not improve\n",
      "Epoch 00018: val_logit_loss did not improve\n",
      "Epoch 00019: val_logit_loss did not improve\n",
      "Epoch 00020: val_logit_loss did not improve\n",
      "Epoch 00021: val_logit_loss did not improve\n",
      "Epoch 00022: val_logit_loss did not improve\n",
      "Epoch 00023: val_logit_loss did not improve\n",
      "Epoch 00024: val_logit_loss did not improve\n",
      "Epoch 00025: val_logit_loss did not improve\n",
      "Epoch 00026: val_logit_loss did not improve\n",
      "Epoch 00027: val_logit_loss did not improve\n",
      "Epoch 00028: val_logit_loss did not improve\n",
      "Epoch 00029: val_logit_loss did not improve\n",
      "Epoch 00030: val_logit_loss did not improve\n",
      "Epoch 00031: val_logit_loss did not improve\n",
      "Epoch 00032: val_logit_loss did not improve\n",
      "Epoch 00033: val_logit_loss did not improve\n",
      "Epoch 00034: val_logit_loss did not improve\n",
      "Epoch 00035: val_logit_loss did not improve\n",
      "Epoch 00036: val_logit_loss did not improve\n",
      "Epoch 00037: val_logit_loss did not improve\n",
      "Epoch 00038: val_logit_loss did not improve\n",
      "Epoch 00039: val_logit_loss did not improve\n",
      "Epoch 00040: val_logit_loss did not improve\n",
      "Epoch 00041: val_logit_loss did not improve\n",
      "Epoch 00042: val_logit_loss did not improve\n",
      "Epoch 00043: val_logit_loss did not improve\n",
      "Epoch 00044: val_logit_loss did not improve\n",
      "Epoch 00045: val_logit_loss did not improve\n",
      "Epoch 00046: val_logit_loss did not improve\n",
      "Epoch 00047: val_logit_loss did not improve\n",
      "Epoch 00048: val_logit_loss did not improve\n",
      "Epoch 00049: val_logit_loss did not improve\n",
      "Epoch 00050: val_logit_loss did not improve\n",
      "Epoch 00051: val_logit_loss did not improve\n",
      "Epoch 00052: val_logit_loss did not improve\n",
      "Epoch 00053: val_logit_loss did not improve\n",
      "Epoch 00054: val_logit_loss did not improve\n",
      "Epoch 00055: val_logit_loss did not improve\n",
      "Epoch 00056: val_logit_loss did not improve\n",
      "Epoch 00057: val_logit_loss did not improve\n",
      "Epoch 00058: val_logit_loss did not improve\n",
      "Epoch 00059: val_logit_loss did not improve\n",
      "Epoch 00060: val_logit_loss did not improve\n",
      "Epoch 00061: val_logit_loss did not improve\n",
      "Epoch 00062: val_logit_loss did not improve\n",
      "Epoch 00063: val_logit_loss did not improve\n",
      "Epoch 00064: val_logit_loss did not improve\n",
      "Epoch 00065: val_logit_loss did not improve\n",
      "Epoch 00066: val_logit_loss did not improve\n",
      "Epoch 00067: val_logit_loss did not improve\n",
      "Epoch 00068: val_logit_loss did not improve\n",
      "Epoch 00069: val_logit_loss did not improve\n",
      "Epoch 00070: val_logit_loss did not improve\n",
      "Epoch 00071: val_logit_loss did not improve\n",
      "Epoch 00072: val_logit_loss did not improve\n",
      "Epoch 00073: val_logit_loss did not improve\n",
      "Epoch 00074: val_logit_loss did not improve\n",
      "Epoch 00075: val_logit_loss did not improve\n",
      "Epoch 00076: val_logit_loss did not improve\n",
      "Epoch 00077: val_logit_loss did not improve\n",
      "Epoch 00078: val_logit_loss did not improve\n",
      "Epoch 00079: val_logit_loss did not improve\n",
      "Epoch 00080: val_logit_loss did not improve\n",
      "Epoch 00081: val_logit_loss did not improve\n",
      "Epoch 00082: val_logit_loss did not improve\n",
      "Epoch 00083: val_logit_loss did not improve\n",
      "Epoch 00084: val_logit_loss did not improve\n",
      "Epoch 00085: val_logit_loss did not improve\n",
      "Epoch 00086: val_logit_loss did not improve\n",
      "Epoch 00087: val_logit_loss did not improve\n",
      "Epoch 00088: val_logit_loss did not improve\n",
      "Epoch 00089: val_logit_loss did not improve\n",
      "Epoch 00090: val_logit_loss did not improve\n",
      "Epoch 00091: val_logit_loss did not improve\n",
      "Epoch 00092: val_logit_loss did not improve\n",
      "Epoch 00093: val_logit_loss did not improve\n",
      "Epoch 00094: val_logit_loss did not improve\n",
      "Epoch 00095: val_logit_loss did not improve\n",
      "Epoch 00096: val_logit_loss did not improve\n",
      "Epoch 00097: val_logit_loss did not improve\n",
      "Epoch 00098: val_logit_loss did not improve\n",
      "Epoch 00099: val_logit_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = prepareData(features)\n",
    "X_train, y_train, X_val, y_val = X_train[train_idx,:], y_train[train_idx,:], X_train[val_idx,:], y_train[val_idx,:]\n",
    "\n",
    "\n",
    "for i in range(0,lp[0].shape[0]):\n",
    "    print(\"\\n\\n\\t First part:\\n\")\n",
    "    auto = Autoencoder(architecture, modelType, weightsDirectory,\n",
    "                       dropout=lp[0][i], inputNoise=lp[3][i], l2reg=lp[1][i], autoencoderLoss=lp[4][i])\n",
    "    auto.buildAutoencoder()\n",
    "        \n",
    "    auto.fit(X_train, y_train, X_val, y_val, epochs=200, batch=lp[2][i])\n",
    "    \n",
    "    chkpt_0 = \"%s/%s.hdf5\"%(auto.wD, auto.name)\n",
    "        \n",
    "    auto.freezeAutoencoder(True)\n",
    "        \n",
    "    print(\"\\n\\n\\t Second part:\\n\"%lp)\n",
    "    auto.loadFromWeights(chkpt_0)\n",
    "    auto.fit(X_train, y_train, X_val, y_val, epochs=100, batch=lp[2][i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on stocks separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testOnSeparateStocks(autoList, feat):\n",
    "    n = len(autoList)\n",
    "    trainNumOfTrades = np.zeros((clf_portfolio_dic.N_stocks,))\n",
    "    valNumOfTrades = np.zeros((clf_portfolio_dic.N_stocks,))\n",
    "    testNumOfTrades = np.zeros((clf_portfolio_dic.N_stocks,))\n",
    "    acc_train_autoencoder = np.zeros((n,clf_portfolio_dic.N_stocks,))\n",
    "    acc_val_autoencoder = np.zeros((n,clf_portfolio_dic.N_stocks,))\n",
    "    acc_test_autoencoder = np.zeros((n,clf_portfolio_dic.N_stocks,))\n",
    "\n",
    "    # difficulty is to recreate the validation set for each stock\n",
    "    indexCount = 0\n",
    "    for k, stock in enumerate(clf_portfolio_dic.stocks):\n",
    "        name = \"%s30\"%stock\n",
    "        print(k,name)\n",
    "        \n",
    "        # extract stock data\n",
    "        x_train = np.array(clf_portfolio_dic.X_train_dic[name][feat])\n",
    "        x_test = np.array(clf_portfolio_dic.X_test_dic[name][feat])\n",
    "        y_train = (clf_portfolio_dic.y_train_dic[name]+1)/2\n",
    "        y_test = (clf_portfolio_dic.y_test_dic[name]+1)/2\n",
    "        \n",
    "        # split train into val+train\n",
    "        n = y_train.shape[0]\n",
    "        mask = np.array([(i+indexCount in val_idx) for i in range(n)])\n",
    "        x_train, y_train, x_val, y_val = x_train[~mask,:], y_train[~mask], x_train[mask,:], y_train[mask]\n",
    "        indexCount += n\n",
    "        assert(n==y_train.shape[0]+y_val.shape[0])\n",
    "\n",
    "        \n",
    "        # transform ys to one-hot vectors\n",
    "        y_t = np.zeros((y_train.shape[0], 2))\n",
    "        y_t[np.arange(y_train.shape[0]), y_train.astype('int32')] = 1\n",
    "        y_train = y_t\n",
    "\n",
    "        y_t = np.zeros((y_val.shape[0], 2))\n",
    "        y_t[np.arange(y_val.shape[0]), y_val.astype('int32')] = 1\n",
    "        y_val = y_t\n",
    "        \n",
    "        y_t = np.zeros((y_test.shape[0], 2))\n",
    "        y_t[np.arange(y_test.shape[0]), y_test.astype('int32')] = 1\n",
    "        y_test = y_t\n",
    "        \n",
    "        # store numbers of trades\n",
    "        trainNumOfTrades[k] = y_train.shape[0]\n",
    "        valNumOfTrades[k] = y_val.shape[0]\n",
    "        testNumOfTrades[k] = y_test.shape[0]\n",
    "    \n",
    "        # compute accuracies\n",
    "        for i, a in enumerate(autoList):\n",
    "            y_pred_train = a.predict(x_train)\n",
    "            y_pred_val = a.predict(x_val)\n",
    "            y_pred_test = a.predict(x_test)\n",
    "            acc_train = (100 * (np.argmax(y_pred_train,1) == np.argmax(y_train,1))).mean()\n",
    "            acc_val = (100 * (np.argmax(y_pred_val,1) == np.argmax(y_val,1))).mean()\n",
    "            acc_test = (100 * (np.argmax(y_pred_test,1) == np.argmax(y_test,1))).mean()\n",
    "            acc_train_autoencoder[i,k] = acc_train\n",
    "            acc_val_autoencoder[i,k] = acc_val\n",
    "            acc_test_autoencoder[i,k] = acc_test\n",
    "        \n",
    "    return(acc_train_autoencoder, acc_val_autoencoder, acc_test_autoencoder,\n",
    "           trainNumOfTrades, valNumOfTrades, testNumOfTrades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0\n",
      "_weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f0.hdf5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 120)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 120)           0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 100)           12100       dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 100)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "encoded (Dense)                  (None, 30)            3030        dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 100)           3100        encoded[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 100)           0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "decoded (Dense)                  (None, 120)           12120       dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "logit (Dense)                    (None, 2)             62          encoded[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 30,412\n",
      "Trainable params: 30,412\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f1\n",
      "_weights/A30S_m1_(120,100,30)_d0.00684_in0.0698_r(0,0.005)_b256_l(cp)_f1.hdf5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 120)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 120)           0           input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 100)           12100       dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 100)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "encoded (Dense)                  (None, 30)            3030        dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 100)           3100        encoded[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 100)           0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "decoded (Dense)                  (None, 120)           12120       dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "logit (Dense)                    (None, 2)             62          encoded[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 30,412\n",
      "Trainable params: 62\n",
      "Non-trainable params: 30,350\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "0 DNB30\n",
      "1 NRG30\n",
      "2 CL30\n",
      "3 ANTM30\n",
      "4 NEE30\n",
      "5 PAYX30\n",
      "6 VAR30\n",
      "7 NI30\n",
      "8 MNST30\n",
      "9 JNJ30\n",
      "10 TGNA30\n",
      "11 NOV30\n",
      "12 FIS30\n",
      "13 BLK30\n",
      "14 HBI30\n",
      "15 NVDA30\n",
      "16 DLTR30\n",
      "17 MRO30\n",
      "18 EMN30\n",
      "19 AMT30\n",
      "20 FLR30\n",
      "21 IBM30\n",
      "22 BK30\n",
      "23 NFX30\n",
      "24 AGN30\n",
      "25 LRCX30\n",
      "26 DIS30\n",
      "27 LH30\n",
      "28 C30\n",
      "29 MNK30\n",
      "\n",
      "62.2439991191 | 60.834292648\n",
      "56.5489735496 | 54.9171269989\n",
      "51.5406836784 | 49.9486074212\n",
      "\n",
      "62.2819251756 | 60.9476404229\n",
      "56.6615282977 | 54.9532777681\n",
      "51.5915048414 | 50.0265232046\n"
     ]
    }
   ],
   "source": [
    "expList = []\n",
    "autoList = []\n",
    "\n",
    "for k, features in enumerate([features_1p4_extra_z]): #features_1p4_extra_a, features_1p4_extra_z\n",
    "    for j,encoding_size in enumerate([30]): #,50,70\n",
    "        architecture = [len(features),100,encoding_size]\n",
    "        if features == features_1p4_extra_a:\n",
    "            feat = \"a\"\n",
    "        elif features == features_1p4_extra_z:\n",
    "            feat = \"z\"\n",
    "        elif features == features_1p4_extra_a_z:\n",
    "            feat = \"a_z\"\n",
    "        lp = listOfParameters[\"%s_%s\"%(feat,encoding_size)]\n",
    "        \n",
    "        # for each run\n",
    "        for i in range(0,lp[0].shape[0]):\n",
    "            for f in [0,1]:\n",
    "                d=lp[0][i]\n",
    "                l2=lp[1][i]\n",
    "                batchSize=lp[2][i]\n",
    "                inputNoise=lp[3][i]\n",
    "                loss= \"cp\" if lp[4][i] == 'cosine_proximity' else \"mse\"\n",
    "                name = \"A30S_m%s_(%s,%s,%s)_d%s_in%s_r(%s,%s)_b%s_l(%s)_f%s\"%(\n",
    "                        modelType, len(features),100, encoding_size, d, inputNoise, 0, l2, batchSize, loss, f)\n",
    "                print(name)\n",
    "                e = ExperimentPerformance(\n",
    "                    methodName=name, \n",
    "                    stocks=stocks, originalFeatures=features)\n",
    "                auto = Autoencoder([len(features), 100, encoding_size], modelType, dropout=d,\n",
    "                                    inputNoise=inputNoise,l2reg=l2,autoencoderLoss=lp[4][i])\n",
    "                auto.buildAutoencoder()\n",
    "                if f == 1:\n",
    "                    auto.freezeAutoencoder(True)\n",
    "                chkpt_0 = \"%s/%s.hdf5\"%(weightsDirectory,name)\n",
    "                auto.loadFromWeights(chkpt_0)\n",
    "                \n",
    "                autoList.append(auto)\n",
    "                expList.append(e)\n",
    "\n",
    "        # Train and Test accuracy on separate stocks        \n",
    "        acc_train_vec, acc_val_vec, acc_test_vec, trainNumOfTrades, valNumOfTrades, testNumOfTrades = testOnSeparateStocks(autoList,features)    \n",
    "        for i, e in enumerate(expList):\n",
    "            e.setTrainResults(acc_train_vec[i,:], trainNumOfTrades)\n",
    "            e.setValResults(acc_val_vec[i,:], valNumOfTrades)\n",
    "            e.setTestResults(acc_test_vec[i,:], testNumOfTrades)\n",
    "            print(\"\")\n",
    "            print(\"%s | %s\"%(e.getAccuracy(set='train'),e.getAccuracyMinusSigma(set='train')))\n",
    "            print(\"%s | %s\"%(e.getAccuracy(set='val'),e.getAccuracyMinusSigma(set='val')))\n",
    "            print(\"%s | %s\"%(e.getAccuracy(set='test'),e.getAccuracyMinusSigma(set='test')))\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(expList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#with open(\"pickles/acc_autoencoder_logit_70_z_denoising.p\",'wb') as f:\n",
    "    pickle.dump( expList, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"_f0_\" in \"weightsCV_run21/test_2017-07-03_15:30:16_z_30_f0_0.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Portfolio\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, STATUS_FAIL, tpe\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1_l2\n",
    "import keras\n",
    "import keras.callbacks\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import methodsMLinterns\n",
    "import customAutoencoder\n",
    "\n",
    "\n",
    "features_1p4_extra_a_z =['aab', 'aac', 'aad', 'aae', 'aaf', 'aag', 'aah', 'abj', 'abm', 'abn', 'abo', 'abp', 'abq', 'abr', 'abs', 'abt', 'abu', 'abv', 'abw', 'abx', 'aby', 'abz', 'aca', 'acb', 'acc', 'acd', 'ace', 'acf', 'acr', 'acw', 'acx', 'acy', 'adi', 'adj', 'adl', 'ado', 'adp', 'adq', 'adr', 'ads', 'adt', 'adu', 'adv', 'adw', 'adx', 'ady', 'adz', 'aea', 'aeb', 'aec', 'aed', 'aee', 'aef', 'aeg', 'aeh', 'aei', 'aej', 'aek', 'ael', 'aem', 'aen', 'aeo', 'aep', 'aeq', 'aer', 'aes', 'aex', 'aey', 'aez', 'afa', 'afj', 'afl', 'afo', 'afp', 'afq', 'afr', 'afs', 'aft', 'afu', 'afv', 'afw', 'afx', 'afy', 'afz', 'aga', 'agb', 'agc', 'agd', 'age', 'agf', 'agg', 'agh', 'agi', 'agj', 'agk', 'agl', 'agm', 'agn', 'ago', 'agp', 'agq', 'agr', 'ags', 'agt', 'agu', 'agv', 'agw', 'agx', 'agy', 'ahf', 'ahg', 'ahh', 'ahi', 'ahj', 'ahk', 'ahl', 'ahm', 'ahn', 'aho', 'zhq', 'zhr', 'zhs', 'zht', 'zhu', 'zhv', 'zhw', 'ziy', 'zjb', 'zjc', 'zjd', 'zje', 'zjf', 'zjg', 'zjh', 'zji', 'zjj', 'zjk', 'zjl', 'zjm', 'zjn', 'zjo', 'zjp', 'zjq', 'zjr', 'zjs', 'zjt', 'zju', 'zkg', 'zkl', 'zkm', 'zkn', 'zkx', 'zky', 'zla', 'zld', 'zle', 'zlf', 'zlg', 'zlh', 'zli', 'zlj', 'zlk', 'zll', 'zlm', 'zln', 'zlo', 'zlp', 'zlq', 'zlr', 'zls', 'zlt', 'zlu', 'zlv', 'zlw', 'zlx', 'zly', 'zlz', 'zma', 'zmb', 'zmc', 'zmd', 'zme', 'zmf', 'zmg', 'zmh', 'zmm', 'zmn', 'zmo', 'zmp', 'zmy', 'zna', 'znd', 'zne', 'znf', 'zng', 'znh', 'zni', 'znj', 'znk', 'znl', 'znm', 'znn', 'zno', 'znp', 'znq', 'znr', 'zns', 'znt', 'znu', 'znv', 'znw', 'znx', 'zny', 'znz', 'zoa', 'zob', 'zoc', 'zod', 'zoe', 'zof', 'zog', 'zoh', 'zoi', 'zoj', 'zok', 'zol', 'zom', 'zon', 'zou', 'zov', 'zow', 'zox', 'zoy', 'zoz', 'zpa', 'zpb', 'zpc', 'zpd', 'zpe']\n",
    "features_1p4_extra_a=[f for f in features_1p4_extra_a_z if f[0]=='a']\n",
    "features_1p4_extra_z=[f for f in features_1p4_extra_a_z if f[0]=='z']\n",
    "\n",
    "stocks = ['DNB', 'NRG', 'CL', 'ANTM', 'NEE', 'PAYX', 'VAR', 'NI', 'MNST', 'JNJ', 'TGNA', 'NOV', 'FIS', 'BLK', 'HBI', 'NVDA', 'DLTR', 'MRO', 'EMN', 'AMT', 'FLR', 'IBM', 'BK', 'NFX', 'AGN', 'LRCX', 'DIS', 'LH', 'C', 'MNK']\n",
    "    \n",
    "date_test_set = datetime.date(2016, 1, 1)\n",
    "    \n",
    "print(\"Loading Portfolio\")\n",
    "    \n",
    "clf_portfolio_dic = methodsMLinterns.ClassificationPortfolio(stocks=stocks, minutes_forward=30)\n",
    "clf_portfolio_dic.loadData()\n",
    "clf_portfolio_dic.cleanUpData(features_1p4_extra_a_z)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features z and encoding dimension 30\n",
      "Loading Data\n",
      "/usr/local/lib/python3.5/site-packages/hyperas/optim.py\n",
      "/usr/local/lib/python3.5/site-packages/hyperas/optim.py\n",
      "/usr/local/lib/python3.5/site-packages/hyperas/optim.py\n",
      "<ipython-input-5-471320de4d80>\n",
      "<ipython-input-5-471320de4d80>\n",
      "/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\n",
      "/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\n",
      "/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/zmqshell.py\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\n",
      "/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\n",
      "/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\n",
      "/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\n",
      "/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\n",
      "/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\n",
      "/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\n",
      "/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\n",
      "/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py\n",
      "/usr/local/Cellar/python3/3.5.2_2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\n",
      "/usr/local/Cellar/python3/3.5.2_2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/clement/Dropbox (Personal)/ArrowInterns/clement/<ipython-input-5-471320de4d80>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-471320de4d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0mhyperasSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_1p4_extra_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainPercentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightsDir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weightsCV_run21\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museShuffledDays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-471320de4d80>\u001b[0m in \u001b[0;36mhyperasSearchCV\u001b[0;34m(features, encoding_dim, evals, trainPercentage, weightsDir, useValDate, useShuffledDays)\u001b[0m\n\u001b[1;32m    156\u001b[0m                                  \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                                  \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                                  trials=Trials())\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s/CVhyperasSearchBestParameters_%s_%s.p\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweightsDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, rseed, notebook_name, verbose)\u001b[0m\n\u001b[1;32m     40\u001b[0m     best_run = base_minimizer(model=model, data=data, algo=algo, max_evals=max_evals,\n\u001b[1;32m     41\u001b[0m                               \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                               notebook_name=notebook_name, verbose=verbose)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_model_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hyperopt_model_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mtemp_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./temp_model.py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mwrite_temp_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mget_hyperopt_model_string\u001b[0;34m(model, data, notebook_name, verbose, stack)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mcalling_script_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalling_script_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/clement/Dropbox (Personal)/ArrowInterns/clement/<ipython-input-5-471320de4d80>'"
     ]
    }
   ],
   "source": [
    "def data():\n",
    "    with open(\"pickles/CVhyperasSearchConfig.p\",'rb') as f:\n",
    "        x_train, y_train, sets, _, _, _, _ = pickle.load(f)\n",
    "    return x_train, y_train, sets\n",
    "\n",
    "\n",
    "def model(x_train, y_train, sets):\n",
    "    if 'parameters' not in globals():\n",
    "        global parameters\n",
    "        parameters = []\n",
    "    if 'val_loss1' not in globals():\n",
    "        global val_loss1\n",
    "        val_loss1 = []\n",
    "    if 'val_loss2' not in globals():\n",
    "        global val_loss2\n",
    "        val_loss2 = []\n",
    "    parameters.append(space)\n",
    "    print(space)\n",
    "\n",
    "\n",
    "    with open(\"pickles/CVhyperasSearchConfig.p\",'rb') as f:\n",
    "        _, _, _, feat, encod_dim, architecture, weightsDir = pickle.load(f)\n",
    "    print(feat, encod_dim, architecture)\n",
    "    \n",
    "    autoencoderLoss = 'cosine_proximity'\n",
    "    d = {{choice(numpy.arange(10,81,5)/100)}}\n",
    "    l2={{choice([.0025, .005, .01, .02, .04, .08])}}\n",
    "    inputNoise = 0\n",
    "    batch_size = 256\n",
    "    modelType = customAutoencoder.ModelType.Sequential\n",
    "\n",
    "\n",
    "    log_name = datetime.datetime.today().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    \n",
    "    models = []\n",
    "    freezed_models = []\n",
    "    for train, val in sets:\n",
    "        merged_model = customAutoencoder.Autoencoder(architecture, modelType, weightsDirectory=weightsDir, dropout=d, inputNoise=inputNoise, l1reg=0, l2reg=l2, autoencoderLoss=autoencoderLoss)\n",
    "        merged_model.buildAutoencoder()\n",
    "        models.append(merged_model)\n",
    "    \n",
    "    print(\"\\n\\nTraining First Model\\n\\n\")\n",
    "    val_logit_acc = 0\n",
    "    for i, m in enumerate(models):\n",
    "        print(\"\\n\\tFold %s\\n\"%i)\n",
    "        checkpointName0=\"%s/test_%s_%s_%s_f0_%s.hdf5\" % (weightsDir, log_name, feat, encod_dim, i)\n",
    "        train_idx = sets[i][0]\n",
    "        val_idx = sets[i][1]\n",
    "        \n",
    "        m.fit(x_train[train_idx,:], y_train[train_idx,:], x_train[val_idx,:], y_train[val_idx,:], epochs=2, batch=batch_size, checkpointName=checkpointName0)\n",
    "            \n",
    "        vl, _ = m.model1.evaluate(x_train[val_idx,:], x_train[val_idx,:], verbose=0)\n",
    "        val_loss1.append(vl)\n",
    "        if math.isnan(vl):\n",
    "            val_loss2.append(vl)\n",
    "            with open(\"%s/CVhyperasSearchParameters_%s_%s.p\" % (weightsDir, feat, encod_dim), 'wb') as f:\n",
    "                pickle.dump( [parameters,val_loss1,val_loss2], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                return {'loss': 0, 'status': STATUS_FAIL, 'model': None}\n",
    " \n",
    "        m.buildAutoencoder(trainable=False)\n",
    "        m.loadFromWeights(checkpointName0)\n",
    "\n",
    "        freezed_models.append(m)\n",
    "          \n",
    "        _, vlacc = m.model1.evaluate(x_train[val_idx,:], x_train[val_idx,:], verbose=0)\n",
    "        val_logit_acc += vlacc\n",
    "        print(\"VLMAE \",vlacc)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n\\nTraining Freezed Model\\n\\n\")\n",
    "    val_logit_acc_freezed = 0\n",
    "    for i, m in enumerate(freezed_models):\n",
    "        print(\"\\n\\tFold %s\\n\"%i)\n",
    "        checkpointName1=\"%s/test_%s_%s_%s_f1_%s.hdf5\" % (weightsDir, log_name, feat, encod_dim, i)\n",
    "        train_idx = sets[i][0]\n",
    "        val_idx = sets[i][1]\n",
    "        \n",
    "        m.fit(x_train[train_idx,:], y_train[train_idx,:], x_train[val_idx,:], y_train[val_idx,:], epochs=1, batch=batch_size, checkpointName=checkpointName1)\n",
    "        \n",
    "        vl, _ = m.model.evaluate(x_train[val_idx,:], y_train[val_idx,:], verbose=0)\n",
    "        val_loss2.append(vl)\n",
    "        with open(\"%s/CVhyperasSearchParameters_%s_%s.p\" % (weightsDir, feat, encod_dim), 'wb') as f:\n",
    "            pickle.dump( [parameters,val_loss1,val_loss2], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        if math.isnan(vl):\n",
    "            return {'loss': 0, 'status': STATUS_FAIL, 'model': None}\n",
    "\n",
    "        m.loadFromWeights(checkpointName1)\n",
    "\n",
    "        _, vlacc = m.model.evaluate(x_train[val_idx,:], y_train[val_idx,:] , verbose=0)\n",
    "        val_logit_acc_freezed += vlacc\n",
    "        print(\"VLACC \",vlacc)\n",
    "    \n",
    "    return {'loss': -val_logit_acc_freezed, 'status': STATUS_OK, 'model': merged_model}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hyperasSearchCV(features, encoding_dim, evals, trainPercentage=100, weightsDir=\"weightsCV\", useValDate=False, useShuffledDays=False):\n",
    "    if not os.path.isdir(weightsDir):\n",
    "        os.makedirs(weightsDir)\n",
    "    \n",
    "    if not useValDate and not useShuffledDays:\n",
    "        clf_portfolio_dic.getTrainTestSetDate(date_test_set)\n",
    "    elif useValDate and trainPercentage == 80:\n",
    "        clf_portfolio_dic.getTrainValTestSetDate(datetime.date(2015, 5, 1), date_test_set)\n",
    "    elif useValDate and trainPercentage == 90:\n",
    "        clf_portfolio_dic.getTrainValTestSetDate(datetime.date(2015, 8, 24), date_test_set)\n",
    "    elif useShuffledDays and not useValDate:\n",
    "        clf_portfolio_dic.getTrainValTestShuffledDaysSetDate(date_test_set, percentageTrain=trainPercentage)\n",
    "\n",
    "\n",
    "\n",
    "    architecture = [len(features), 100, encoding_dim]\n",
    "    if features == features_1p4_extra_a:\n",
    "        feat = \"a\"\n",
    "    elif features == features_1p4_extra_z:\n",
    "        feat = \"z\"\n",
    "    elif features == features_1p4_extra_a_z:\n",
    "        feat = \"a_z\"\n",
    "\n",
    "    print(\"Features %s and encoding dimension %s\"%(feat,encoding_dim))\n",
    "\n",
    "    def prepareData(features):\n",
    "        x_train = np.array([], dtype=np.float32).reshape(0,len(features))\n",
    "        y_train = np.array([], dtype=np.float32).reshape(0,1)\n",
    "    \n",
    "        for k, stock in enumerate(clf_portfolio_dic.stocks):\n",
    "            name = stock + str(clf_portfolio_dic.minutes_forward)\n",
    "            if k==0:\n",
    "                x_train, y_train = clf_portfolio_dic.X_train_dic[name][features].as_matrix(),(clf_portfolio_dic.y_train_dic[name]+1)/2\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,clf_portfolio_dic.X_train_dic[name][features].as_matrix()),axis=0)\n",
    "                y_train = np.concatenate((y_train,(clf_portfolio_dic.y_train_dic[name]+1)/2),axis=0)\n",
    "\n",
    "        y_t = np.zeros((y_train.shape[0], 2))\n",
    "        y_t[np.arange(y_train.shape[0]), y_train.astype('int32')] = 1\n",
    "        y_train = y_t\n",
    "    \n",
    "        return x_train,y_train\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Loading Data\")\n",
    "    assert(trainPercentage in [80,90,100])\n",
    "    with open(\"pickles/CV_setsIndexes_%sp.p\"%trainPercentage,'rb') as f:\n",
    "        sets = pickle.load(f)\n",
    "   \n",
    "    X_train, y_train = prepareData(features)\n",
    "\n",
    "    with open(\"pickles/CVhyperasSearchConfig.p\",'wb') as f:\n",
    "        pickle.dump( [X_train, y_train, sets, feat, encoding_dim, architecture, weightsDir], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    best_run, _ = optim.minimize(model=model,\n",
    "                                 data=data,\n",
    "                                 algo=tpe.suggest,\n",
    "                                 max_evals=evals,\n",
    "                                 trials=Trials())\n",
    "\n",
    "    with open(\"%s/CVhyperasSearchBestParameters_%s_%s.p\" % (weightsDir, feat, encoding_dim),'wb') as f:\n",
    "        pickle.dump( best_run, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(\"%s/CVhyperasSearchParameters_%s_%s.p\" % (weightsDir, feat, encoding_dim),'rb') as f:\n",
    "        parameters,val_loss1,val_loss2 = pickle.load(f)\n",
    "    print(\"----------\")\n",
    "    for p in parameters:\n",
    "        print(p)\n",
    "    print(\"----------\")\n",
    "    for v in val_loss1:\n",
    "        print(v)\n",
    "    print(\"----------\")\n",
    "    for v in val_loss2:\n",
    "        print(v)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hyperasSearchCV(features_1p4_extra_z, 30, evals=2, trainPercentage=90, weightsDir=\"weightsCV_run21\", useShuffledDays=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
